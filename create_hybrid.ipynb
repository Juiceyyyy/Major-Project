{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Model for Human Detection and Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, TimeDistributed, Conv2D, MaxPooling2D, Flatten\n",
    "from keras.applications import VGG16\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Paths Setup\n",
    "base_dir = os.path.abspath('dataset/personpath22')\n",
    "video_path = os.path.join(base_dir, 'raw_data')\n",
    "annotation_dir = os.path.join(base_dir, 'annotation')\n",
    "processed_data_path = os.path.join(base_dir, 'hybrid_processed_data.npz')\n",
    "\n",
    "# Annotation Files\n",
    "amodal_file = os.path.join(annotation_dir, 'anno_amodal_2022.json')\n",
    "visible_file = os.path.join(annotation_dir, 'anno_visible_2022.json')\n",
    "splits_file = os.path.join(annotation_dir, 'splits.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available video files: ['uid_vid_00000.mp4', 'uid_vid_00001.mp4', 'uid_vid_00002.mp4', 'uid_vid_00003.mp4', 'uid_vid_00004.mp4', 'uid_vid_00005.mp4', 'uid_vid_00006.mp4', 'uid_vid_00007.mp4', 'uid_vid_00008.mp4', 'uid_vid_00009.mp4', 'uid_vid_00010.mp4', 'uid_vid_00011.mp4', 'uid_vid_00012.mp4', 'uid_vid_00013.mp4', 'uid_vid_00014.mp4', 'uid_vid_00015.mp4', 'uid_vid_00016.mp4', 'uid_vid_00017.mp4', 'uid_vid_00018.mp4', 'uid_vid_00019.mp4', 'uid_vid_00020.mp4', 'uid_vid_00021.mp4', 'uid_vid_00022.mp4', 'uid_vid_00023.mp4', 'uid_vid_00024.mp4', 'uid_vid_00025.mp4', 'uid_vid_00026.mp4', 'uid_vid_00027.mp4', 'uid_vid_00028.mp4', 'uid_vid_00029.mp4', 'uid_vid_00030.mp4', 'uid_vid_00031.mp4', 'uid_vid_00032.mp4', 'uid_vid_00033.mp4', 'uid_vid_00034.mp4', 'uid_vid_00035.mp4', 'uid_vid_00036.mp4', 'uid_vid_00037.mp4', 'uid_vid_00038.mp4', 'uid_vid_00039.mp4', 'uid_vid_00040.mp4', 'uid_vid_00041.mp4', 'uid_vid_00042.mp4', 'uid_vid_00043.mp4', 'uid_vid_00044.mp4', 'uid_vid_00045.mp4', 'uid_vid_00046.mp4', 'uid_vid_00047.mp4', 'uid_vid_00048.mp4', 'uid_vid_00049.mp4', 'uid_vid_00050.mp4', 'uid_vid_00051.mp4', 'uid_vid_00052.mp4', 'uid_vid_00053.mp4', 'uid_vid_00054.mp4', 'uid_vid_00055.mp4', 'uid_vid_00056.mp4', 'uid_vid_00057.mp4', 'uid_vid_00058.mp4', 'uid_vid_00059.mp4', 'uid_vid_00060.mp4', 'uid_vid_00061.mp4', 'uid_vid_00062.mp4', 'uid_vid_00063.mp4', 'uid_vid_00064.mp4', 'uid_vid_00065.mp4', 'uid_vid_00066.mp4', 'uid_vid_00067.mp4', 'uid_vid_00068.mp4', 'uid_vid_00069.mp4', 'uid_vid_00070.mp4', 'uid_vid_00071.mp4', 'uid_vid_00072.mp4', 'uid_vid_00073.mp4', 'uid_vid_00074.mp4', 'uid_vid_00075.mp4', 'uid_vid_00076.mp4', 'uid_vid_00077.mp4', 'uid_vid_00078.mp4', 'uid_vid_00079.mp4', 'uid_vid_00080.mp4', 'uid_vid_00081.mp4', 'uid_vid_00082.mp4', 'uid_vid_00083.mp4', 'uid_vid_00084.mp4', 'uid_vid_00085.mp4', 'uid_vid_00086.mp4', 'uid_vid_00087.mp4', 'uid_vid_00088.mp4', 'uid_vid_00089.mp4', 'uid_vid_00090.mp4', 'uid_vid_00091.mp4', 'uid_vid_00092.mp4', 'uid_vid_00093.mp4', 'uid_vid_00094.mp4', 'uid_vid_00095.mp4', 'uid_vid_00096.mp4', 'uid_vid_00097.mp4', 'uid_vid_00098.mp4', 'uid_vid_00099.mp4', 'uid_vid_00100.mp4', 'uid_vid_00101.mp4', 'uid_vid_00144.mp4', 'uid_vid_00145.mp4', 'uid_vid_00146.mp4', 'uid_vid_00147.mp4', 'uid_vid_00148.mp4', 'uid_vid_00149.mp4', 'uid_vid_00150.mp4', 'uid_vid_00151.mp4', 'uid_vid_00152.mp4', 'uid_vid_00153.mp4', 'uid_vid_00154.mp4', 'uid_vid_00155.mp4', 'uid_vid_00156.mp4', 'uid_vid_00157.mp4', 'uid_vid_00158.mp4', 'uid_vid_00159.mp4', 'uid_vid_00160.mp4', 'uid_vid_00161.mp4', 'uid_vid_00162.mp4', 'uid_vid_00163.mp4', 'uid_vid_00164.mp4', 'uid_vid_00165.mp4', 'uid_vid_00166.mp4', 'uid_vid_00167.mp4', 'uid_vid_00168.mp4', 'uid_vid_00169.mp4', 'uid_vid_00170.mp4', 'uid_vid_00171.mp4', 'uid_vid_00172.mp4', 'uid_vid_00173.mp4', 'uid_vid_00174.mp4', 'uid_vid_00175.mp4', 'uid_vid_00176.mp4', 'uid_vid_00177.mp4', 'uid_vid_00178.mp4', 'uid_vid_00179.mp4', 'uid_vid_00180.mp4', 'uid_vid_00181.mp4', 'uid_vid_00182.mp4', 'uid_vid_00183.mp4', 'uid_vid_00184.mp4', 'uid_vid_00185.mp4', 'uid_vid_00186.mp4', 'uid_vid_00187.mp4', 'uid_vid_00188.mp4', 'uid_vid_00189.mp4', 'uid_vid_00190.mp4', 'uid_vid_00191.mp4', 'uid_vid_00192.mp4', 'uid_vid_00193.mp4', 'uid_vid_00194.mp4', 'uid_vid_00195.mp4', 'uid_vid_00196.mp4', 'uid_vid_00197.mp4', 'uid_vid_00198.mp4', 'uid_vid_00199.mp4', 'uid_vid_00200.mp4', 'uid_vid_00201.mp4', 'uid_vid_00202.mp4', 'uid_vid_00203.mp4', 'uid_vid_00204.mp4', 'uid_vid_00205.mp4', 'uid_vid_00206.mp4', 'uid_vid_00207.mp4', 'uid_vid_00208.mp4', 'uid_vid_00209.mp4', 'uid_vid_00210.mp4', 'uid_vid_00211.mp4', 'uid_vid_00212.mp4', 'uid_vid_00213.mp4', 'uid_vid_00214.mp4', 'uid_vid_00215.mp4', 'uid_vid_00216.mp4', 'uid_vid_00217.mp4', 'uid_vid_00218.mp4', 'uid_vid_00219.mp4', 'uid_vid_00220.mp4', 'uid_vid_00221.mp4', 'uid_vid_00222.mp4', 'uid_vid_00223.mp4', 'uid_vid_00224.mp4', 'uid_vid_00225.mp4', 'uid_vid_00226.mp4', 'uid_vid_00227.mp4', 'uid_vid_00228.mp4', 'uid_vid_00229.mp4', 'uid_vid_00230.mp4', 'uid_vid_00231.mp4', 'uid_vid_00232.mp4', 'uid_vid_00233.mp4', 'uid_vid_00234.mp4', 'uid_vid_00235.mp4', 'uid_vid_00236.mp4', 'uid_vid_99999.mp4']\n",
      "Train videos: ['uid_vid_99999.mp4', 'uid_vid_00000.mp4', 'uid_vid_00001.mp4', 'uid_vid_00002.mp4', 'uid_vid_00003.mp4', 'uid_vid_00004.mp4', 'uid_vid_00005.mp4', 'uid_vid_00006.mp4', 'uid_vid_00007.mp4', 'uid_vid_00010.mp4', 'uid_vid_00012.mp4', 'uid_vid_00014.mp4', 'uid_vid_00015.mp4', 'uid_vid_00016.mp4', 'uid_vid_00017.mp4', 'uid_vid_00021.mp4', 'uid_vid_00022.mp4', 'uid_vid_00023.mp4', 'uid_vid_00025.mp4', 'uid_vid_00026.mp4', 'uid_vid_00027.mp4', 'uid_vid_00029.mp4', 'uid_vid_00032.mp4', 'uid_vid_00033.mp4', 'uid_vid_00034.mp4', 'uid_vid_00037.mp4', 'uid_vid_00039.mp4', 'uid_vid_00040.mp4', 'uid_vid_00041.mp4', 'uid_vid_00042.mp4', 'uid_vid_00044.mp4', 'uid_vid_00047.mp4', 'uid_vid_00049.mp4', 'uid_vid_00050.mp4', 'uid_vid_00052.mp4', 'uid_vid_00053.mp4', 'uid_vid_00054.mp4', 'uid_vid_00055.mp4', 'uid_vid_00058.mp4', 'uid_vid_00059.mp4', 'uid_vid_00060.mp4', 'uid_vid_00061.mp4', 'uid_vid_00062.mp4', 'uid_vid_00065.mp4', 'uid_vid_00070.mp4', 'uid_vid_00072.mp4', 'uid_vid_00073.mp4', 'uid_vid_00074.mp4', 'uid_vid_00075.mp4', 'uid_vid_00077.mp4', 'uid_vid_00081.mp4', 'uid_vid_00083.mp4', 'uid_vid_00084.mp4', 'uid_vid_00088.mp4', 'uid_vid_00089.mp4', 'uid_vid_00091.mp4', 'uid_vid_00093.mp4', 'uid_vid_00094.mp4', 'uid_vid_00095.mp4', 'uid_vid_00097.mp4', 'uid_vid_00101.mp4', 'uid_vid_00103.mp4', 'uid_vid_00104.mp4', 'uid_vid_00106.mp4', 'uid_vid_00108.mp4', 'uid_vid_00110.mp4', 'uid_vid_00111.mp4', 'uid_vid_00112.mp4', 'uid_vid_00115.mp4', 'uid_vid_00116.mp4', 'uid_vid_00145.mp4', 'uid_vid_00146.mp4', 'uid_vid_00148.mp4', 'uid_vid_00151.mp4', 'uid_vid_00152.mp4', 'uid_vid_00119.mp4', 'uid_vid_00120.mp4', 'uid_vid_00123.mp4', 'uid_vid_00128.mp4', 'uid_vid_00129.mp4', 'uid_vid_00131.mp4', 'uid_vid_00132.mp4', 'uid_vid_00134.mp4', 'uid_vid_00135.mp4', 'uid_vid_00136.mp4', 'uid_vid_00137.mp4', 'uid_vid_00140.mp4', 'uid_vid_00142.mp4', 'uid_vid_00143.mp4', 'uid_vid_00154.mp4', 'uid_vid_00155.mp4', 'uid_vid_00156.mp4', 'uid_vid_00157.mp4', 'uid_vid_00159.mp4', 'uid_vid_00160.mp4', 'uid_vid_00164.mp4', 'uid_vid_00165.mp4', 'uid_vid_00168.mp4', 'uid_vid_00171.mp4', 'uid_vid_00176.mp4', 'uid_vid_00177.mp4', 'uid_vid_00180.mp4', 'uid_vid_00181.mp4', 'uid_vid_00182.mp4', 'uid_vid_00184.mp4', 'uid_vid_00185.mp4', 'uid_vid_00186.mp4', 'uid_vid_00187.mp4', 'uid_vid_00188.mp4', 'uid_vid_00192.mp4', 'uid_vid_00194.mp4', 'uid_vid_00195.mp4', 'uid_vid_00196.mp4', 'uid_vid_00197.mp4', 'uid_vid_00199.mp4', 'uid_vid_00202.mp4', 'uid_vid_00203.mp4', 'uid_vid_00204.mp4', 'uid_vid_00206.mp4', 'uid_vid_00208.mp4', 'uid_vid_00209.mp4', 'uid_vid_00210.mp4', 'uid_vid_00211.mp4', 'uid_vid_00213.mp4', 'uid_vid_00214.mp4', 'uid_vid_00215.mp4', 'uid_vid_00216.mp4', 'uid_vid_00217.mp4', 'uid_vid_00220.mp4', 'uid_vid_00223.mp4', 'uid_vid_00224.mp4', 'uid_vid_00225.mp4', 'uid_vid_00227.mp4', 'uid_vid_00229.mp4', 'uid_vid_00231.mp4', 'uid_vid_00232.mp4', 'uid_vid_00233.mp4', 'uid_vid_00236.mp4']\n",
      "Test videos: ['uid_vid_00008.mp4', 'uid_vid_00009.mp4', 'uid_vid_00011.mp4', 'uid_vid_00013.mp4', 'uid_vid_00018.mp4', 'uid_vid_00019.mp4', 'uid_vid_00020.mp4', 'uid_vid_00024.mp4', 'uid_vid_00028.mp4', 'uid_vid_00030.mp4', 'uid_vid_00031.mp4', 'uid_vid_00035.mp4', 'uid_vid_00036.mp4', 'uid_vid_00038.mp4', 'uid_vid_00043.mp4', 'uid_vid_00045.mp4', 'uid_vid_00046.mp4', 'uid_vid_00048.mp4', 'uid_vid_00051.mp4', 'uid_vid_00056.mp4', 'uid_vid_00057.mp4', 'uid_vid_00063.mp4', 'uid_vid_00064.mp4', 'uid_vid_00066.mp4', 'uid_vid_00067.mp4', 'uid_vid_00068.mp4', 'uid_vid_00069.mp4', 'uid_vid_00071.mp4', 'uid_vid_00076.mp4', 'uid_vid_00078.mp4', 'uid_vid_00079.mp4', 'uid_vid_00080.mp4', 'uid_vid_00082.mp4', 'uid_vid_00085.mp4', 'uid_vid_00086.mp4', 'uid_vid_00087.mp4', 'uid_vid_00090.mp4', 'uid_vid_00092.mp4', 'uid_vid_00096.mp4', 'uid_vid_00098.mp4', 'uid_vid_00099.mp4', 'uid_vid_00100.mp4', 'uid_vid_00102.mp4', 'uid_vid_00105.mp4', 'uid_vid_00107.mp4', 'uid_vid_00109.mp4', 'uid_vid_00113.mp4', 'uid_vid_00114.mp4', 'uid_vid_00117.mp4', 'uid_vid_00144.mp4', 'uid_vid_00147.mp4', 'uid_vid_00149.mp4', 'uid_vid_00150.mp4', 'uid_vid_00118.mp4', 'uid_vid_00121.mp4', 'uid_vid_00122.mp4', 'uid_vid_00124.mp4', 'uid_vid_00125.mp4', 'uid_vid_00126.mp4', 'uid_vid_00127.mp4', 'uid_vid_00130.mp4', 'uid_vid_00133.mp4', 'uid_vid_00141.mp4', 'uid_vid_00153.mp4', 'uid_vid_00158.mp4', 'uid_vid_00161.mp4', 'uid_vid_00163.mp4', 'uid_vid_00166.mp4', 'uid_vid_00167.mp4', 'uid_vid_00169.mp4', 'uid_vid_00170.mp4', 'uid_vid_00172.mp4', 'uid_vid_00173.mp4', 'uid_vid_00174.mp4', 'uid_vid_00175.mp4', 'uid_vid_00178.mp4', 'uid_vid_00179.mp4', 'uid_vid_00183.mp4', 'uid_vid_00189.mp4', 'uid_vid_00190.mp4', 'uid_vid_00191.mp4', 'uid_vid_00193.mp4', 'uid_vid_00198.mp4', 'uid_vid_00200.mp4', 'uid_vid_00201.mp4', 'uid_vid_00205.mp4', 'uid_vid_00207.mp4', 'uid_vid_00212.mp4', 'uid_vid_00218.mp4', 'uid_vid_00219.mp4', 'uid_vid_00221.mp4', 'uid_vid_00222.mp4', 'uid_vid_00226.mp4', 'uid_vid_00228.mp4', 'uid_vid_00230.mp4', 'uid_vid_00162.mp4', 'uid_vid_00234.mp4', 'uid_vid_00235.mp4']\n",
      "Loading annotations...\n",
      "Annotations loaded.\n"
     ]
    }
   ],
   "source": [
    "# %% Load Video Files\n",
    "video_files = [f for f in os.listdir(video_path) if f.endswith('.mp4')]\n",
    "print('Available video files:', video_files)\n",
    "\n",
    "# Load splits.json to define train and test sets\n",
    "with open(splits_file, 'r') as f:\n",
    "    splits = json.load(f)\n",
    "train_videos = splits.get('train', [])\n",
    "test_videos = splits.get('test', [])\n",
    "\n",
    "print('Train videos:', train_videos)\n",
    "print('Test videos:', test_videos)\n",
    "\n",
    "# Load Annotations\n",
    "def load_annotations():\n",
    "    \"\"\"Load annotations from amodal and visible files.\"\"\"\n",
    "    print(\"Loading annotations...\")\n",
    "    with open(amodal_file, 'r') as f:\n",
    "        amodal_data = json.load(f)\n",
    "    with open(visible_file, 'r') as f:\n",
    "        visible_data = json.load(f)\n",
    "    print(\"Annotations loaded.\")\n",
    "    return amodal_data, visible_data\n",
    "\n",
    "amodal_data, visible_data = load_annotations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract annotation data for a specific video\n",
    "# %% Extract Annotation Data for Specific Videos\n",
    "def get_annotations(video_name):\n",
    "    \"\"\"Retrieve annotation data for a specific video.\"\"\"\n",
    "    amodal_annos, visible_annos = [], []\n",
    "\n",
    "    individual_amodal_file = os.path.join(annotation_dir, 'anno_amodal_2022', f'{video_name}.json')\n",
    "    individual_visible_file = os.path.join(annotation_dir, 'anno_visible_2022', f'{video_name}.json')\n",
    "\n",
    "    if os.path.exists(individual_amodal_file):\n",
    "        with open(individual_amodal_file, 'r') as f:\n",
    "            amodal_annos = json.load(f).get('entities', [])\n",
    "    if os.path.exists(individual_visible_file):\n",
    "        with open(individual_visible_file, 'r') as f:\n",
    "            visible_annos = json.load(f).get('entities', [])\n",
    "\n",
    "    return amodal_annos, visible_annos\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Initialize Pretrained VGG16 Model for Feature Extraction\n",
    "cnn_model = VGG16(weights='imagenet', include_top=False, input_shape=(32, 32, 3))\n",
    "\n",
    "def extract_features_from_frame(frame):\n",
    "    \"\"\"Extract CNN features from a single frame.\"\"\"\n",
    "    frame = preprocess_input(frame)  # Preprocess input for VGG16\n",
    "    frame = np.expand_dims(frame, axis=0)  # Add batch dimension\n",
    "    features = cnn_model.predict(frame)  # Extract features\n",
    "    return features.reshape(-1)  # Flatten the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Preprocess Videos with CNN Features\n",
    "def preprocess_data_with_cnn(video_file, amodal_annos, visible_annos):\n",
    "    \"\"\"Preprocess video frames and annotations.\"\"\"\n",
    "    print(f\"Processing video: {video_file}...\")\n",
    "    cap = cv2.VideoCapture(video_file)\n",
    "    frames, labels = [], []\n",
    "    frame_labels = {}\n",
    "\n",
    "    # Map frame indices to 'person' labels from annotations\n",
    "    for entity in amodal_annos + visible_annos:\n",
    "        frame_idx = entity['blob']['frame_idx']\n",
    "        label = entity['labels'].get('reflection', 0)  # Example: Use 'reflection' as label\n",
    "        frame_labels[frame_idx] = label\n",
    "\n",
    "    frame_idx = 0\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Resize frame to 32x32 and extract CNN features\n",
    "        frame = cv2.resize(frame, (32, 32))\n",
    "        features = extract_features_from_frame(frame)\n",
    "        frames.append(features)\n",
    "\n",
    "        # Assign label for the current frame\n",
    "        label = frame_labels.get(frame_idx, 0)\n",
    "        labels.append(label)\n",
    "\n",
    "        frame_idx += 1\n",
    "\n",
    "    cap.release()\n",
    "    return np.array(frames), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Create Time-Series Data for LSTM\n",
    "def create_dataset_with_cnn(frames, labels, time_step=10):\n",
    "    \"\"\"Create time-series data from CNN features.\"\"\"\n",
    "    X, y = [], []\n",
    "    for i in range(len(frames) - time_step):\n",
    "        X.append(frames[i:i + time_step])  # Shape: (time_step, features)\n",
    "        y.append(labels[i + time_step])\n",
    "    return np.array(X), np.array(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading processed data...\n",
      "Train Data Shape: X=(89967, 10, 3072), y=(89967,)\n",
      "Test Data Shape: X=(61736, 10, 3072), y=(61736,)\n"
     ]
    }
   ],
   "source": [
    "# %% Load Dataset Based on Split\n",
    "def load_dataset(video_list):\n",
    "    \"\"\"Load all videos and their annotations.\"\"\"\n",
    "    X, y = [], []\n",
    "\n",
    "    for video_name in video_list:\n",
    "        video_file = os.path.join(video_path, video_name)\n",
    "        amodal_annos, visible_annos = get_annotations(video_name)\n",
    "\n",
    "        # Preprocess video and annotations\n",
    "        frames, labels = preprocess_data_with_cnn(video_file, amodal_annos, visible_annos)\n",
    "\n",
    "        # Create time-series data for LSTM\n",
    "        X_video, y_video = create_dataset_with_cnn(frames, labels)\n",
    "        X.extend(X_video)\n",
    "        y.extend(y_video)\n",
    "\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# %% Load or Preprocess Data\n",
    "def load_or_process_data():\n",
    "    \"\"\"Load processed data if available, otherwise process and save datasets.\"\"\"\n",
    "    if os.path.exists(processed_data_path):\n",
    "        print(\"Loading processed data...\")\n",
    "        processed_data = np.load(processed_data_path)\n",
    "        X_train, y_train = processed_data['X_train'], processed_data['y_train']\n",
    "        X_test, y_test = processed_data['X_test'], processed_data['y_test']\n",
    "    else:\n",
    "        print(\"Processing training and testing datasets...\")\n",
    "        X_train, y_train = load_dataset(train_videos)\n",
    "        X_test, y_test = load_dataset(test_videos)\n",
    "\n",
    "        # Save processed data to disk\n",
    "        np.savez(processed_data_path, X_train=X_train, y_train=y_train, \n",
    "                 X_test=X_test, y_test=y_test)\n",
    "        print(f\"Processed data saved to {processed_data_path}\")\n",
    "\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "# Load or process the data\n",
    "X_train, y_train, X_test, y_test = load_or_process_data()\n",
    "\n",
    "# Display the shapes of the datasets\n",
    "print(f'Train Data Shape: X={X_train.shape}, y={y_train.shape}')\n",
    "print(f'Test Data Shape: X={X_test.shape}, y={y_test.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Joshua Menezes\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\wrapper.py:27: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ time_distributed                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>) │           <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ time_distributed_1              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>) │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ time_distributed_2              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7200</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)             │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,450,200</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">51</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ time_distributed                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m32\u001b[0m) │           \u001b[38;5;34m896\u001b[0m │\n",
       "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ time_distributed_1              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m32\u001b[0m) │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ time_distributed_2              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m7200\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)             │     \u001b[38;5;34m1,450,200\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m51\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,451,147</span> (5.54 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,451,147\u001b[0m (5.54 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,451,147</span> (5.54 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,451,147\u001b[0m (5.54 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %% Build CNN + LSTM Model\n",
    "model = Sequential()\n",
    "model.add(TimeDistributed(Conv2D(32, (3, 3), activation='relu'), input_shape=(10, 32, 32, 3)))\n",
    "model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "model.add(TimeDistributed(Flatten()))\n",
    "model.add(LSTM(50, return_sequences=False))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling Sequential.call().\n\n\u001b[1mInvalid input shape for input Tensor(\"sequential_1/Cast:0\", shape=(None, 10, 3072), dtype=float32). Expected shape (None, 10, 32, 32, 3), but input has incompatible shape (None, 10, 3072)\u001b[0m\n\nArguments received by Sequential.call():\n  • inputs=tf.Tensor(shape=(None, 10, 3072), dtype=uint8)\n  • training=True\n  • mask=None",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# %% Train the Model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Plot Training and Validation Performance\u001b[39;00m\n\u001b[0;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\Joshua Menezes\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\Joshua Menezes\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\models\\functional.py:264\u001b[0m, in \u001b[0;36mFunctional._adjust_input_rank\u001b[1;34m(self, flat_inputs)\u001b[0m\n\u001b[0;32m    262\u001b[0m             adjusted\u001b[38;5;241m.\u001b[39mappend(ops\u001b[38;5;241m.\u001b[39mexpand_dims(x, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m    263\u001b[0m             \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m--> 264\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    265\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid input shape for input \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Expected shape \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    266\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mref_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but input has incompatible shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    267\u001b[0m     )\n\u001b[0;32m    268\u001b[0m \u001b[38;5;66;03m# Add back metadata.\u001b[39;00m\n\u001b[0;32m    269\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(flat_inputs)):\n",
      "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling Sequential.call().\n\n\u001b[1mInvalid input shape for input Tensor(\"sequential_1/Cast:0\", shape=(None, 10, 3072), dtype=float32). Expected shape (None, 10, 32, 32, 3), but input has incompatible shape (None, 10, 3072)\u001b[0m\n\nArguments received by Sequential.call():\n  • inputs=tf.Tensor(shape=(None, 10, 3072), dtype=uint8)\n  • training=True\n  • mask=None"
     ]
    }
   ],
   "source": [
    "# %% Train the Model\n",
    "history = model.fit(X_train, y_train, batch_size=32, epochs=10, validation_split=0.2)\n",
    "\n",
    "# Plot Training and Validation Performance\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Accuracy Plot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# Loss Plot\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Save the Model\n",
    "model_json = model.to_json()\n",
    "with open(\"hybrid.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "model.save(\"hybrid.h5\")\n",
    "\n",
    "print(\"Model saved to 'hybrid.h5'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
