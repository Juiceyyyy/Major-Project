{"cells":[{"cell_type":"markdown","metadata":{},"source":["# LSTM Model for Human Detection and Tracking"]},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[],"source":["import os\n","import cv2\n","import numpy as np\n","import json\n","import matplotlib.pyplot as plt\n","from keras.models import Sequential\n","from keras.layers import LSTM, Dense, Dropout\n","from sklearn.model_selection import train_test_split\n"]},{"cell_type":"markdown","metadata":{},"source":["## Load Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Set base directory and paths\n","base_dir = os.path.abspath('dataset/personpath22')\n","video_path = os.path.join(base_dir, 'raw_data')\n","annotation_dir = os.path.join(base_dir, 'annotation')\n","\n","# Load video files\n","video_files = [f for f in os.listdir(video_path) if f.endswith('.mp4')]\n","print('Video files:', video_files)\n"]},{"cell_type":"markdown","metadata":{},"source":["## Load Annotations"]},{"cell_type":"code","execution_count":36,"metadata":{},"outputs":[],"source":["annotations = {}\n","for video in video_files:\n","    uid = video.split('.')[0]\n","    \n","    # Define possible annotation paths\n","    annotation_paths = [\n","        os.path.join(annotation_dir, 'anno_amodal_2022', uid + '.mp4.json'),\n","        os.path.join(annotation_dir, 'anno_visible_2022', uid + '.mp4.json')\n","    ]\n","    \n","    # Try to load annotations from both possible paths\n","    loaded = False\n","    for annotation_path in annotation_paths:\n","        if os.path.exists(annotation_path):\n","            try:\n","                with open(annotation_path, 'r') as f:\n","                    annotations[uid] = json.load(f)\n","                loaded = True\n","                break\n","            except json.JSONDecodeError:\n","                print(f'Error decoding JSON for {uid}. Skipping.')\n","    \n","    if not loaded:\n","        print(f'Annotation file not found for {uid}. Skipping.')\n"]},{"cell_type":"markdown","metadata":{},"source":["## Preprocess Data"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def preprocess_data(video_file, annotation):\n","    # Load video\n","    cap = cv2.VideoCapture(video_file)\n","    frames = []\n","    labels = []\n","\n","    # Extract all frame indices with their corresponding labels\n","    frame_labels = {entity['blob']['frame_idx']: entity['labels'].get('person', 0)\n","                    for entity in annotation['entities']}\n","\n","    # Read video frames\n","    frame_idx = 0\n","    while cap.isOpened():\n","        ret, frame = cap.read()\n","        if not ret:\n","            break\n","\n","        # Resize frame to 64x64 for consistency\n","        frame = cv2.resize(frame, (64, 64))\n","        frames.append(frame)\n","\n","        # Get the label for the current frame\n","        label = frame_labels.get(frame_idx, 0)\n","        labels.append(label)\n","\n","        frame_idx += 1\n","\n","    cap.release()\n","    frames = np.array(frames)\n","    labels = np.array(labels)\n","\n","    return frames, labels\n","\n","example_video = os.path.join(video_path, video_files[0])\n","video_id = video_files[0].split('.')[0]\n","frames, labels = preprocess_data(example_video, annotations[video_id])\n","\n","print('Frames shape:', frames.shape)\n","print('Labels shape:', labels.shape)\n","print('Labels:', labels)\n"]},{"cell_type":"code","execution_count":38,"metadata":{},"outputs":[],"source":["def load_annotation_data(file_path):\n","    \"\"\"Load annotation data from the given file path.\"\"\"\n","    if not os.path.exists(file_path):\n","        raise FileNotFoundError(f\"Annotation file not found: {file_path}\")\n","    \n","    with open(file_path, 'r') as f:\n","        return json.load(f)  # Assuming annotation data is in JSON format\n"]},{"cell_type":"code","execution_count":39,"metadata":{},"outputs":[],"source":["def extract_features_from_annotation(annotation_data):\n","    \"\"\"Extract features from annotation data (modify based on data structure).\"\"\"\n","    print(\"Extracting features from annotation data:\", annotation_data)  # Debug line\n","    if not annotation_data:\n","        print(\"No annotation data available.\")\n","        return np.array([])  # Return an empty array if data is not available\n","    \n","    # Example placeholder logic â€“ modify based on the actual structure of your annotation data\n","    features = annotation_data.get('features', [])\n","    if len(features) == 0:\n","        print(\"No features found in annotation data.\")\n","    return np.array(features)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def load_dataset():\n","    \"\"\"Load features and labels for all videos in the dataset.\"\"\"\n","    X, y = [], []\n","\n","    print(\"Loading annotations...\")\n","\n","    # Iterate through video files\n","    for video_file in video_files:\n","        video_name = os.path.splitext(video_file)[0]\n","        \n","        # Define paths to the annotation directories\n","        amodel_path = os.path.join(annotation_dir, 'anno_amodal_2022', video_name + '.mp4.json')\n","        visible_path = os.path.join(annotation_dir, 'anno_visible_2022', video_name + '.mp4.json')\n","\n","        print(f\"Trying to load annotations for: {video_name}\")\n","        print(f\"Amodal Path: {amodel_path}\")\n","        print(f\"Visible Path: {visible_path}\")\n","\n","        # Attempt to load annotation data\n","        try:\n","            amodel_data = load_annotation_data(amodel_path)\n","            visible_data = load_annotation_data(visible_path)\n","        except FileNotFoundError as e:\n","            print(f\"Skipping {video_file}: {e}\")\n","            continue\n","        except json.JSONDecodeError:\n","            print(f\"Error decoding JSON for {video_name}. Skipping.\")\n","            continue\n","\n","        amodel_features = extract_features_from_annotation(amodel_data)\n","        visible_features = extract_features_from_annotation(visible_data)\n","\n","        # Print the shapes of the extracted features\n","        print(f\"Amodal Features Shape for {video_name}: {amodel_features.shape}\")\n","        print(f\"Visible Features Shape for {video_name}: {visible_features.shape}\")\n","\n","        # Check if features are 1D, if so, reshape them to 2D\n","        if amodel_features.ndim == 1:\n","            amodel_features = amodel_features.reshape(-1, 1)\n","        if visible_features.ndim == 1:\n","            visible_features = visible_features.reshape(-1, 1)\n","\n","        try:\n","            combined_features = np.concatenate([amodel_features, visible_features], axis=1)\n","        except ValueError as e:\n","            print(f\"Feature shape mismatch for {video_name}: {e}\")\n","            continue\n","\n","        X.append(combined_features[:-1])  # Input sequence\n","        y.append(combined_features[1:])   # Predict next timestep\n","\n","    return np.array(X, dtype=object), np.array(y, dtype=object)\n","\n","# Load dataset\n","X, y = load_dataset()\n","print(\"Dataset loaded successfully\")\n","print(f\"X shape: {X.shape}, y shape: {y.shape}\")\n"]},{"cell_type":"markdown","metadata":{},"source":["## Prepare Training Data"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Prepare the data for LSTM\n","def create_dataset(frames, labels, time_step=10):\n","    X, y = [], []\n","    for i in range(len(frames) - time_step):\n","        # Reshape the frames for LSTM input (combine height and width into a single dimension)\n","        X.append(frames[i:i + time_step].reshape(time_step, -1))  # Reshape to (time_step, features)\n","        y.append(labels[i + time_step])  # Adjust based on your label structure\n","    return np.array(X), np.array(y)\n","\n","# Assuming 'frames' and 'labels' are already defined\n","X, y = create_dataset(frames, labels)\n","\n","# Check the new shapes\n","print('X shape:', X.shape)  # Should be (num_samples, time_step, features)\n","print('y shape:', y.shape)  # Adjust as needed"]},{"cell_type":"markdown","metadata":{},"source":["## Build LSTM Model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model = Sequential()\n","model.add(LSTM(50, input_shape=(X.shape[1], X.shape[2])))\n","model.add(Dropout(0.2))\n","model.add(Dense(1, activation='sigmoid'))  # For binary classification\n","model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","model.summary()\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(f'X shape: {X.shape}, y shape: {y.shape}')\n","print('Any NaN in X:', np.isnan(X).any())\n","print('Any NaN in y:', np.isnan(y).any())\n"]},{"cell_type":"markdown","metadata":{},"source":["## Train the Model"]},{"cell_type":"code","execution_count":44,"metadata":{},"outputs":[],"source":["y = y.reshape(-1, 1)  # Reshape to (418, 1) if necessary\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Fit the model\n","history = model.fit(X, y, batch_size=32, epochs=10, validation_split=0.2)\n","\n","# Plot training history\n","import matplotlib.pyplot as plt\n","\n","plt.plot(history.history['accuracy'], label='accuracy')\n","plt.plot(history.history['val_accuracy'], label='val_accuracy')\n","plt.xlabel('Epoch')\n","plt.ylabel('Accuracy')\n","plt.legend(loc='upper left')\n","plt.title('Model Accuracy')\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{},"source":["## Evaluate the Model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import numpy as np\n","\n","# Assuming X and y are your full dataset features and labels\n","# For example:\n","# X = np.random.rand(500, 10, 64, 64, 3)  # Full dataset with 500 samples\n","# y = np.random.randint(0, 2, size=(500,))  # Corresponding labels\n","\n","# Set the random seed for reproducibility\n","np.random.seed(42)\n","\n","# Number of test samples you want to randomly select\n","num_test_samples = 84  # Adjust as needed\n","\n","# Randomly select indices for test data\n","test_indices = np.random.choice(X.shape[0], size=num_test_samples, replace=False)\n","\n","# Create the test datasets\n","X_test = X[test_indices]\n","y_test = y[test_indices]\n","\n","# Optionally, remove the test data from the training set\n","X_train = np.delete(X, test_indices, axis=0)\n","y_train = np.delete(y, test_indices, axis=0)\n","\n","# Print the shapes of the new datasets\n","print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n","print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n","\n","loss, accuracy = model.evaluate(X_test, y_test)\n","print('Test Loss:', loss)\n","print('Test Accuracy:', accuracy)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Save the entire model (architecture + weights + optimizer configuration)\n","model.save('human_detection_model.keras')\n","print(\"Model saved to human_detection_model.keras\")\n","\n","model.summary()"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.7"}},"nbformat":4,"nbformat_minor":4}
